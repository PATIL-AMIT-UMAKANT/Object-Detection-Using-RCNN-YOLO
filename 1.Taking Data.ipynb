{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1nSd-oD1D-c"
   },
   "source": [
    "The main topic of these Notebook is single object detection, which means getting a model to draw a box around every key object in an image, and label each one correctly.  we can use transfer learning from an <b>ImageNet</b> classifier that was never even trained to do detection.We are going to follow these steps:\n",
    "- Classify the biggest object\n",
    "- Predict Bounding box of biggest object\n",
    "- Put all together to predict and classify the biggest object\n",
    "\n",
    "weâ€™ll use a single model to do both these at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7k6gWGn1488"
   },
   "source": [
    "###upgrade and import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T10:32:38.670476Z",
     "iopub.status.busy": "2022-02-01T10:32:38.669857Z",
     "iopub.status.idle": "2022-02-01T10:33:21.089181Z",
     "shell.execute_reply": "2022-02-01T10:33:21.088301Z",
     "shell.execute_reply.started": "2022-02-01T10:32:38.670347Z"
    },
    "id": "sqlUDU2HtKmo",
    "outputId": "f2463851-178a-4864-c967-fcde7c109e70"
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade git+https://github.com/keras-team/keras.git \\\n",
    "                        git+https://github.com/valeoai/dl_utils.git \\\n",
    "                        imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T10:33:26.994572Z",
     "iopub.status.busy": "2022-02-01T10:33:26.993959Z",
     "iopub.status.idle": "2022-02-01T10:33:27.008062Z",
     "shell.execute_reply": "2022-02-01T10:33:27.007233Z",
     "shell.execute_reply.started": "2022-02-01T10:33:26.994519Z"
    },
    "id": "mfIL6Mjiuaos"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.layers import Dropout, Flatten,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import image,patches,patheffects\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E0RP_jy2igW"
   },
   "source": [
    "## Pascal VOC\n",
    "We will be looking at the [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset. It's quite slow, so you may prefer to download from [this mirror](https://pjreddie.com/projects/pascal-voc-dataset-mirror/). There are two different competition/research datasets, from 2007 and 2012. We'll be using the 2007 version. later we can use the larger 2012 for better results.\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "if not os.path.isdir('./VOCdevkit/'):\n",
    "    ! wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
    "    ! tar -xf VOCtrainval_06-Nov-2007.tar\n",
    "    #! wget https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip\n",
    "    #! unzip -q PASCAL_VOC.zip -d VOCdevkit/VOC2007\n",
    "    #! rm -Rf VOCdevkit/VOC2007/PASCAL_VOC\n",
    "    !mv  ../input/pascal-voc-2007/PASCAL_VOC/PASCAL_VOC/pascal_train2012.json VOCdevkit/VOC2007/pascal_train2007.json\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T10:27:42.828084Z",
     "iopub.status.busy": "2022-01-31T10:27:42.827706Z",
     "iopub.status.idle": "2022-01-31T10:27:43.638558Z",
     "shell.execute_reply": "2022-01-31T10:27:43.637485Z",
     "shell.execute_reply.started": "2022-01-31T10:27:42.828037Z"
    }
   },
   "outputs": [],
   "source": [
    " !mv  ../input/pascal-voc-2007/PASCAL_VOC/PASCAL_VOC/pascal_train2007.json VOCdevkit/VOC2007/pascal_train2007.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:30:04.730990Z",
     "iopub.status.busy": "2022-02-01T11:30:04.730571Z",
     "iopub.status.idle": "2022-02-01T11:30:04.748002Z",
     "shell.execute_reply": "2022-02-01T11:30:04.747103Z",
     "shell.execute_reply.started": "2022-02-01T11:30:04.730960Z"
    },
    "id": "r6J3HWkku1bK",
    "outputId": "81066050-cd8d-40ba-bb44-3434f470e8d6"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH = Path('../input/pascal-voc-2007/PASCAL_VOC/PASCAL_VOC')\n",
    "for i in PATH.iterdir(): print(i)\n",
    "    \n",
    "JPEGS = PATH/'JPEGImages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufT57-mC4MAS"
   },
   "source": [
    "so, you have:\n",
    "- <kbd>JPEGImages</kbd> folder containing all the images\n",
    "- <kbd>Annotations</kbd> folder containing all the annotation, one XML annotation file per image file\n",
    "\n",
    "The original version were in XML, which is a little hard to work with nowadays, so we use the more recent JSON version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:30:04.750164Z",
     "iopub.status.busy": "2022-02-01T11:30:04.749728Z",
     "iopub.status.idle": "2022-02-01T11:30:04.922991Z",
     "shell.execute_reply": "2022-02-01T11:30:04.922012Z",
     "shell.execute_reply.started": "2022-02-01T11:30:04.750125Z"
    },
    "id": "wbQzS4ObwmWS",
    "outputId": "a90d292a-3bf4-4ab0-deef-57b3a35284a2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "BD = json.load((PATH/'pascal_train2007.json').open()) # it loads a dictionary\n",
    "print('the dictionary of keys: ',BD.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:31:52.905570Z",
     "iopub.status.busy": "2022-02-01T11:31:52.904937Z",
     "iopub.status.idle": "2022-02-01T11:31:52.911055Z",
     "shell.execute_reply": "2022-02-01T11:31:52.910466Z",
     "shell.execute_reply.started": "2022-02-01T11:31:52.905539Z"
    },
    "id": "y4k1fYCIwmWZ",
    "outputId": "3720e5f9-cc24-4f64-bea9-01d525774f88"
   },
   "outputs": [],
   "source": [
    "# Then, we can acces to the image\n",
    "BD['images'][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:31:30.146977Z",
     "iopub.status.busy": "2022-02-01T11:31:30.146251Z",
     "iopub.status.idle": "2022-02-01T11:31:30.154027Z",
     "shell.execute_reply": "2022-02-01T11:31:30.153179Z",
     "shell.execute_reply.started": "2022-02-01T11:31:30.146940Z"
    },
    "id": "Aw0KksTvu1lq",
    "outputId": "665767cb-afd7-4fa8-e62d-985f61f8dde1"
   },
   "outputs": [],
   "source": [
    "# The annotations\n",
    "BD['annotations'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:32:40.620472Z",
     "iopub.status.busy": "2022-02-01T11:32:40.619840Z",
     "iopub.status.idle": "2022-02-01T11:32:40.628099Z",
     "shell.execute_reply": "2022-02-01T11:32:40.627305Z",
     "shell.execute_reply.started": "2022-02-01T11:32:40.620429Z"
    },
    "id": "UUAF32Icu1rH",
    "outputId": "57d8d1bf-8916-43f0-cc8e-996d7977b590"
   },
   "outputs": [],
   "source": [
    "# and all category\n",
    "BD['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:41:23.190681Z",
     "iopub.status.busy": "2022-02-01T11:41:23.189984Z",
     "iopub.status.idle": "2022-02-01T11:41:23.195511Z",
     "shell.execute_reply": "2022-02-01T11:41:23.194741Z",
     "shell.execute_reply.started": "2022-02-01T11:41:23.190644Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:44:19.907786Z",
     "iopub.status.busy": "2022-02-01T11:44:19.907105Z",
     "iopub.status.idle": "2022-02-01T11:44:19.929676Z",
     "shell.execute_reply": "2022-02-01T11:44:19.929077Z",
     "shell.execute_reply.started": "2022-02-01T11:44:19.907753Z"
    }
   },
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(BD['images'])\n",
    "df1.to_csv('images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:46:28.343924Z",
     "iopub.status.busy": "2022-02-01T11:46:28.343150Z",
     "iopub.status.idle": "2022-02-01T11:46:28.436975Z",
     "shell.execute_reply": "2022-02-01T11:46:28.436270Z",
     "shell.execute_reply.started": "2022-02-01T11:46:28.343883Z"
    }
   },
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(BD['annotations'])\n",
    "df2.to_csv('annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:47:50.025501Z",
     "iopub.status.busy": "2022-02-01T11:47:50.024987Z",
     "iopub.status.idle": "2022-02-01T11:47:50.032517Z",
     "shell.execute_reply": "2022-02-01T11:47:50.031700Z",
     "shell.execute_reply.started": "2022-02-01T11:47:50.025453Z"
    }
   },
   "outputs": [],
   "source": [
    "df3=pd.DataFrame(BD['categories'])\n",
    "df3.to_csv('categories.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
